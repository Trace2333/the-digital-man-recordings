# Seq2Seq搭建总结

## 一、总结

除了结构，影响训练和精度最重要的是词嵌入。

词嵌入转换的时候一般有多种方式，目前个人认为最为有效的是利用词典反向建立embedding表并利用embedding表来进行词嵌入，但此方法也存在一定的问题，即未登录词的构建。一般来说我们倾向于使用随机初始化来得到未登录词，而一个语料库若未登录词的数量比较大，会造成词嵌入带有随机性，无法真实反映词分布，依据其完成的参数迭代容易产生过拟合。

在训练中，seq2seq独有的一个训练指导方式是以输出指导输入。

将目标（Target）序列作为decoder的输入来指导参数更新而非使用encoder的输入来作为decoder的相应输入。在预测的时候则简单利用encoder的输出来作为下一个decoder的输出即可。

由于它的这个特性，最终的生成效果极大依赖预测阶段的输出情况和输出评估矩阵。

还有一个尚未使用到的技巧，叫loss-mask。

（可以基于pytorch的第三方API来进行操作，需要packedSequence等）

## 二、数据准备

### 1.数据类型

**模型处理的数据标准格式是句子对。**

举个例子，一对标准输入可以是["你好！我叫Trace！"]和["Hello！My name is Trace！"]

### 2.数据处理

数据无论源语言还是目标语言都应该进行分词处理，形成词对，分词不一定是要一个字符作为一个token，而是应该以一个翻译单元作为一个token，如上文中的(你好)与(Hello)是一个标准对。

### 3.词嵌入

词嵌入在这里有两种常用方法，即反向词典法和预训练法。

#### 反向词典法

反向词典采用第三方大规模训练的词嵌入文档进行token级的查找然后建立词嵌入文档来形成词嵌入matrix。此方法的词嵌入在训练期间是一个动态矩阵，与参数具有同等效力，在训练期间随着参数更新的step进行更新。

缺点是在训练的时候参数矩阵的更新是整体化更新，并且在矩阵建立期间的未登陆词汇处理使用的是随机初始化法，导致部分参数是处于一种不确定状态，更新参数期间会破坏掉原第三方所建立的词关系。

#### 预训练法

预训练法采用语言模型如（word2vec，glove等）进行建模处理，形成词嵌入matrix，之后再将此词嵌入矩阵输入到模型进行embedding。

优点是词向量更加符合规律和符合条件，但是由于词表自身的限制，会导致大量的未登录词还有狭小的语言规律范围，导致词向量泛用性有限

#### 混合法

可以采用将第三方库与语料库结合的方法，即进行增量训练——这是目前最好的方法，但是由于词表的巨大，有时甚至比模型本身参数量更大，这时也会很大程度上拖累迭代速度。

## 三、模型构建

### 1.RNN Base

### 2.RNN with Attention

### 3.Attention Hybrid

